{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 — All-in-One Particle Tracking & Diffusion Analysis\n",
    "\n",
    "**PHYS 382 Advanced Lab — Microscopy & Motility**\n",
    "\n",
    "This notebook performs the complete analysis pipeline:\n",
    "1. **Particle tracking** from `.avi` video (temporal median background, connected components, Hungarian linking)\n",
    "2. **Track segmentation** (split at jumps, filter short/stationary tracks)\n",
    "3. **Trajectory visualization** in physical units\n",
    "4. **Displacement histogram** with Gaussian fit → D (variance & Gaussian methods)\n",
    "5. **MSD analysis** with linear fit → D (MSD slope method) and power-law fit → α exponent\n",
    "6. **Stokes-Einstein comparison** with viscosity adjusted for glycerol concentration\n",
    "7. **Summary** with all results saved to disk\n",
    "\n",
    "### Usage\n",
    "1. Set `AVI_PATH` and experiment parameters in Cell 2\n",
    "2. **Run All Cells** (Ctrl+Shift+Enter or Cell → Run All)\n",
    "3. Figures auto-saved to `Analysis/figures/{date}/{filename}/`\n",
    "4. Re-running deletes old figures and creates fresh ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CELL 2 — USER INPUTS (CHANGE THESE FOR EACH VIDEO)\n# ============================================================================\n\n# Paste the full path to your .avi video file\nAVI_PATH = r\"D:\\Documents\\SFU\\PHYS382-AdvancedLab\\phys332w-sfu-GIT\\phys332W-sfu\\Lab2-Microscopy-and-Motility\\Data\\2026-02-24\\3um-0_5p-4_9pstock-0pgly-trial1.avi\"\n\n# --- Experiment metadata (change per video) ---\nBEAD_DIAMETER_UM = 3.0        # Bead diameter in micrometers\nGLYCEROL_PERCENT = 0.0        # Glycerol weight percentage (0 = pure water)\nPIXEL_SIZE = 0.0684           # um/px (68.4 nm/px, 100x oil — confirmed Sessions 1,3,4)\nFRAME_RATE = 226              # fps (frames per second)\nTEMPERATURE_C = 21.0          # Celsius\n\n# --- Tracking parameters (usually don't need changing) ---\nDETECTION_THRESHOLD = 15      # Intensity threshold above background (0-255)\nGAUSSIAN_BLUR_SIGMA = 1.5     # Gaussian blur sigma for noise reduction\nMIN_PARTICLE_AREA = 4         # Minimum connected component area (px^2)\nMAX_PARTICLE_AREA = 200       # Maximum connected component area (px^2)\nMAX_DISPLACEMENT = 10         # Maximum linking distance (px/frame)\nMAX_GAP_FRAMES = 3            # Gap-closing up to N frames\nMIN_TRACK_LENGTH = 10         # Minimum track length to keep (frames)\nMIN_TOTAL_DISPLACEMENT = 3.0  # Minimum net displacement to keep (px)\n\n# --- Analysis parameters ---\nNUM_BEST_SEGMENTS = 10        # Number of best track segments to use\nMIN_SEGMENT_LENGTH = 10       # Minimum segment length after splitting (frames)\nMAX_JUMP_PX = 20              # Split tracks at jumps larger than this (px)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3 — IMPORTS & FIGURE DIRECTORY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy.optimize import linear_sum_assignment, curve_fit\n",
    "from math import sqrt, pi\n",
    "\n",
    "# --- Physical constants ---\n",
    "k_B = 1.381e-23               # Boltzmann constant (J/K)\n",
    "TEMPERATURE = TEMPERATURE_C + 273.15  # Kelvin\n",
    "\n",
    "# --- Parse AVI path to extract date folder and filename ---\n",
    "avi_path = Path(AVI_PATH)\n",
    "if not avi_path.exists():\n",
    "    raise FileNotFoundError(f\"AVI file not found: {AVI_PATH}\")\n",
    "\n",
    "parts = avi_path.parts\n",
    "idx = [i for i, p in enumerate(parts) if p.lower() == 'data']\n",
    "if idx:\n",
    "    date_folder = parts[idx[-1] + 1]   # e.g. \"24-Feb\" or \"2026-02-05\"\n",
    "else:\n",
    "    date_folder = avi_path.parent.name\n",
    "file_stem = avi_path.stem              # e.g. \"3um-0_5p-4_9pstock-0pGly-trail1\"\n",
    "\n",
    "# --- Set up figure output directory ---\n",
    "# Figures dir is relative to this notebook's location (Analysis/)\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "FIGURES_DIR = NOTEBOOK_DIR / 'figures' / date_folder / file_stem\n",
    "\n",
    "# Delete old figures and create fresh directory\n",
    "if FIGURES_DIR.exists():\n",
    "    shutil.rmtree(FIGURES_DIR)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Viscosity calculation (Cheng 2008 formula for glycerol-water mixtures) ---\n",
    "def get_viscosity(glycerol_pct, temp_c):\n",
    "    \"\"\"Empirical viscosity for glycerol-water mixture (Pa.s).\n",
    "    Uses Cheng (2008) correlation for glycerol-water mixtures.\n",
    "    Valid for 0-100% glycerol, 0-100 C.\n",
    "    \"\"\"\n",
    "    T = temp_c\n",
    "    # Water viscosity (Vogel equation, mPa.s)\n",
    "    a_w = 1.790\n",
    "    eta_water = a_w * np.exp((-1230 - T) * T / (36100 + 360 * T))\n",
    "    \n",
    "    if glycerol_pct <= 0:\n",
    "        return eta_water * 1e-3  # Convert mPa.s to Pa.s\n",
    "    \n",
    "    # Glycerol viscosity (mPa.s)\n",
    "    a_g = 12100\n",
    "    eta_glycerol = a_g * np.exp((-1233 + T) * T / (9900 + 70 * T))\n",
    "    \n",
    "    # Cheng mixing rule\n",
    "    cm = glycerol_pct / 100.0  # mass fraction\n",
    "    alpha_cheng = 0.705 - 0.0017 * T\n",
    "    a_mix = cm ** alpha_cheng * (1 - cm) ** (1 - alpha_cheng)\n",
    "    \n",
    "    ln_eta = cm * np.log(eta_glycerol) + (1 - cm) * np.log(eta_water) \\\n",
    "             + a_mix * cm * (1 - cm)\n",
    "    \n",
    "    return np.exp(ln_eta) * 1e-3  # Convert mPa.s to Pa.s\n",
    "\n",
    "VISCOSITY = get_viscosity(GLYCEROL_PERCENT, TEMPERATURE_C)\n",
    "dt = 1.0 / FRAME_RATE\n",
    "\n",
    "# --- Print configuration summary ---\n",
    "print('=' * 70)\n",
    "print('LAB 2 — PARTICLE TRACKING & DIFFUSION ANALYSIS PIPELINE')\n",
    "print('=' * 70)\n",
    "print(f'\\nVideo: {avi_path.name}')\n",
    "print(f'  Date folder: {date_folder}')\n",
    "print(f'  Full path: {AVI_PATH}')\n",
    "print(f'\\nExperiment parameters:')\n",
    "print(f'  Bead diameter: {BEAD_DIAMETER_UM} um')\n",
    "print(f'  Glycerol: {GLYCEROL_PERCENT}%')\n",
    "print(f'  Pixel size: {PIXEL_SIZE} um/px ({PIXEL_SIZE*1000:.1f} nm/px)')\n",
    "print(f'  Frame rate: {FRAME_RATE} fps (dt = {dt:.6f} s)')\n",
    "print(f'  Temperature: {TEMPERATURE_C} C ({TEMPERATURE:.2f} K)')\n",
    "print(f'  Viscosity: {VISCOSITY:.6f} Pa.s')\n",
    "print(f'\\nFigures will be saved to:')\n",
    "print(f'  {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4 — PARTICLE TRACKING ENGINE\n",
    "# (Functions from track_onion_particles.py)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_temporal_median(cap, sample_every=10):\n",
    "    \"\"\"Compute per-pixel temporal median from sampled frames.\"\"\"\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames_sampled = []\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    for i in range(0, total_frames, sample_every):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n",
    "        frames_sampled.append(gray.astype(np.float32))\n",
    "    if len(frames_sampled) == 0:\n",
    "        raise RuntimeError(\"Could not read any frames from video\")\n",
    "    print(f\"  Sampled {len(frames_sampled)} frames for background estimation\")\n",
    "    background = np.median(np.stack(frames_sampled), axis=0).astype(np.uint8)\n",
    "    return background\n",
    "\n",
    "\n",
    "def detect_particles(gray_frame, background, threshold, blur_sigma,\n",
    "                     min_area, max_area):\n",
    "    \"\"\"Detect particles by background subtraction + thresholding.\"\"\"\n",
    "    ksize = int(blur_sigma * 4) | 1\n",
    "    blurred = cv2.GaussianBlur(gray_frame, (ksize, ksize), blur_sigma)\n",
    "    diff = cv2.absdiff(blurred, background)\n",
    "    _, binary = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    n_labels, labels, stats, centroids_cv = cv2.connectedComponentsWithStats(\n",
    "        binary, connectivity=8)\n",
    "    particles = []\n",
    "    for label_id in range(1, n_labels):\n",
    "        area = stats[label_id, cv2.CC_STAT_AREA]\n",
    "        if area < min_area or area > max_area:\n",
    "            continue\n",
    "        mask = (labels == label_id)\n",
    "        weights = diff[mask].astype(np.float64)\n",
    "        weight_sum = weights.sum()\n",
    "        if weight_sum > 0:\n",
    "            ys, xs = np.where(mask)\n",
    "            cx = np.average(xs.astype(np.float64), weights=weights)\n",
    "            cy = np.average(ys.astype(np.float64), weights=weights)\n",
    "        else:\n",
    "            cx = centroids_cv[label_id, 0]\n",
    "            cy = centroids_cv[label_id, 1]\n",
    "        particles.append((cx, cy))\n",
    "    return particles, binary\n",
    "\n",
    "\n",
    "def track_all_frames(detections_per_frame, max_disp, max_gap, min_length):\n",
    "    \"\"\"Link detections across all frames into tracks (Hungarian algorithm).\"\"\"\n",
    "    all_tracks = []\n",
    "    active_tracks = []\n",
    "    next_id = 1\n",
    "    frame_numbers = sorted(detections_per_frame.keys())\n",
    "    \n",
    "    for frame_num in frame_numbers:\n",
    "        curr_detections = detections_per_frame[frame_num]\n",
    "        prev_positions = []\n",
    "        prev_track_indices = []\n",
    "        prev_gap_sizes = []\n",
    "        \n",
    "        for i, track in enumerate(active_tracks):\n",
    "            gap = frame_num - track['last_seen_frame']\n",
    "            if gap <= max_gap + 1:\n",
    "                pos = track['positions'][track['last_seen_frame']]\n",
    "                prev_positions.append(pos)\n",
    "                prev_track_indices.append(i)\n",
    "                prev_gap_sizes.append(gap)\n",
    "        \n",
    "        effective_max_disps = [max_disp * g for g in prev_gap_sizes]\n",
    "        \n",
    "        if len(prev_positions) > 0 and len(curr_detections) > 0:\n",
    "            n_prev = len(prev_positions)\n",
    "            n_curr = len(curr_detections)\n",
    "            INF = 1e9\n",
    "            cost = np.full((n_prev, n_curr), INF)\n",
    "            for i, (px, py) in enumerate(prev_positions):\n",
    "                for j, (cx, cy) in enumerate(curr_detections):\n",
    "                    d = np.sqrt((px - cx)**2 + (py - cy)**2)\n",
    "                    if d <= effective_max_disps[i]:\n",
    "                        cost[i, j] = d\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "            matches = {}\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                if cost[r, c] < INF:\n",
    "                    matches[r] = c\n",
    "            matched_curr = set(matches.values())\n",
    "            unmatched_curr = [j for j in range(n_curr) if j not in matched_curr]\n",
    "        else:\n",
    "            matches = {}\n",
    "            unmatched_curr = list(range(len(curr_detections)))\n",
    "        \n",
    "        for prev_idx, curr_idx in matches.items():\n",
    "            track_idx = prev_track_indices[prev_idx]\n",
    "            track = active_tracks[track_idx]\n",
    "            gap = frame_num - track['last_seen_frame']\n",
    "            track['positions'][frame_num] = curr_detections[curr_idx]\n",
    "            track['flags'][frame_num] = '*' if gap > 1 else ' '\n",
    "            track['last_seen_frame'] = frame_num\n",
    "            track['end_frame'] = frame_num\n",
    "        \n",
    "        still_active = []\n",
    "        for i, track in enumerate(active_tracks):\n",
    "            if frame_num - track['last_seen_frame'] > max_gap:\n",
    "                all_tracks.append(track)\n",
    "            else:\n",
    "                still_active.append(track)\n",
    "        active_tracks = still_active\n",
    "        \n",
    "        for curr_idx in unmatched_curr:\n",
    "            new_track = {\n",
    "                'id': next_id,\n",
    "                'positions': {frame_num: curr_detections[curr_idx]},\n",
    "                'flags': {frame_num: ' '},\n",
    "                'start_frame': frame_num,\n",
    "                'end_frame': frame_num,\n",
    "                'last_seen_frame': frame_num,\n",
    "            }\n",
    "            active_tracks.append(new_track)\n",
    "            next_id += 1\n",
    "    \n",
    "    all_tracks.extend(active_tracks)\n",
    "    return all_tracks\n",
    "\n",
    "\n",
    "def clean_tracks(tracks, min_length, min_displacement):\n",
    "    \"\"\"Remove spurious tracks (too short or stationary).\"\"\"\n",
    "    cleaned = []\n",
    "    for track in tracks:\n",
    "        frames = sorted(track['positions'].keys())\n",
    "        if len(frames) < min_length:\n",
    "            continue\n",
    "        first = track['positions'][frames[0]]\n",
    "        last = track['positions'][frames[-1]]\n",
    "        net_disp = np.sqrt((last[0] - first[0])**2 + (last[1] - first[1])**2)\n",
    "        if net_disp < min_displacement:\n",
    "            continue\n",
    "        cleaned.append(track)\n",
    "    for i, track in enumerate(cleaned, 1):\n",
    "        track['id'] = i\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def write_mtrack2_format(tracks, total_frames, output_path):\n",
    "    \"\"\"Write tracks in MTrack2-compatible tab-separated format.\"\"\"\n",
    "    n_tracks = len(tracks)\n",
    "    with open(output_path, 'w') as f:\n",
    "        header_parts = ['Frame']\n",
    "        for i in range(1, n_tracks + 1):\n",
    "            header_parts.extend([f'X{i}', f'Y{i}', f'Flag{i}'])\n",
    "        f.write('\\t'.join(header_parts) + '\\n')\n",
    "        f.write(f'Tracks 1 to {n_tracks}\\n')\n",
    "        for frame in range(1, total_frames + 1):\n",
    "            row_parts = [str(frame)]\n",
    "            for track in tracks:\n",
    "                if frame in track['positions']:\n",
    "                    x, y = track['positions'][frame]\n",
    "                    flag = track['flags'].get(frame, ' ')\n",
    "                    row_parts.extend([f'{x:.5f}', f'{y:.5f}', flag])\n",
    "                else:\n",
    "                    row_parts.extend([' ', ' ', ' '])\n",
    "            f.write('\\t'.join(row_parts) + '\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('Track \\tLength\\tDistance traveled\\tNr of Frames\\n')\n",
    "        for i, track in enumerate(tracks, 1):\n",
    "            frames = sorted(track['positions'].keys())\n",
    "            path_length = 0.0\n",
    "            for j in range(1, len(frames)):\n",
    "                p1 = track['positions'][frames[j - 1]]\n",
    "                p2 = track['positions'][frames[j]]\n",
    "                path_length += np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
    "            start = track['positions'][frames[0]]\n",
    "            end = track['positions'][frames[-1]]\n",
    "            distance = np.sqrt((end[0] - start[0])**2 + (end[1] - start[1])**2)\n",
    "            f.write(f'{i}\\t{path_length:.5f}\\t{distance:.5f}\\t{len(frames)}\\n')\n",
    "\n",
    "\n",
    "# ======================== RUN TRACKING PIPELINE ========================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STEP 1: PARTICLE TRACKING')\n",
    "print('=' * 70)\n",
    "\n",
    "cap = cv2.VideoCapture(AVI_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open video: {AVI_PATH}\")\n",
    "\n",
    "fps_meta = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f'\\nVideo info:')\n",
    "print(f'  Resolution: {width} x {height}')\n",
    "print(f'  Total frames: {total_frames}')\n",
    "print(f'  Metadata FPS: {fps_meta}')\n",
    "print(f'  Using FRAME_RATE: {FRAME_RATE} fps')\n",
    "print(f'  Duration: {total_frames / FRAME_RATE:.2f} s')\n",
    "\n",
    "# --- Background estimation ---\n",
    "print(f'\\nComputing temporal median background...')\n",
    "t0 = time.time()\n",
    "background = compute_temporal_median(cap, sample_every=10)\n",
    "print(f'  Done in {time.time() - t0:.1f} s')\n",
    "\n",
    "# --- Detect particles per frame ---\n",
    "print(f'\\nDetecting particles in {total_frames} frames...')\n",
    "t0 = time.time()\n",
    "detections_per_frame = {}\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "for frame_num in range(1, total_frames + 1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        total_frames = frame_num - 1\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n",
    "    particles, _ = detect_particles(gray, background, DETECTION_THRESHOLD,\n",
    "                                     GAUSSIAN_BLUR_SIGMA, MIN_PARTICLE_AREA,\n",
    "                                     MAX_PARTICLE_AREA)\n",
    "    detections_per_frame[frame_num] = particles\n",
    "    if frame_num % 500 == 0 or frame_num == 1:\n",
    "        print(f'  Frame {frame_num}/{total_frames}: {len(particles)} particles')\n",
    "\n",
    "cap.release()\n",
    "elapsed = time.time() - t0\n",
    "print(f'  Detection complete in {elapsed:.1f} s ({total_frames / max(elapsed, 0.01):.0f} fps)')\n",
    "\n",
    "counts = [len(detections_per_frame.get(f, [])) for f in range(1, total_frames + 1)]\n",
    "print(f'  Particles per frame: min={min(counts)}, max={max(counts)}, mean={np.mean(counts):.1f}')\n",
    "\n",
    "# --- Link into tracks ---\n",
    "print(f'\\nLinking particles into tracks...')\n",
    "t0 = time.time()\n",
    "raw_tracks = track_all_frames(detections_per_frame, MAX_DISPLACEMENT, MAX_GAP_FRAMES, 1)\n",
    "print(f'  Raw tracks: {len(raw_tracks)}')\n",
    "\n",
    "tracks = clean_tracks(raw_tracks, MIN_TRACK_LENGTH, MIN_TOTAL_DISPLACEMENT)\n",
    "print(f'  Valid tracks after cleaning: {len(tracks)}')\n",
    "print(f'  Tracking complete in {time.time() - t0:.1f} s')\n",
    "\n",
    "if len(tracks) == 0:\n",
    "    raise RuntimeError(\"No valid tracks found! Try adjusting DETECTION_THRESHOLD, \"\n",
    "                       \"MIN_TRACK_LENGTH, or MAX_DISPLACEMENT.\")\n",
    "\n",
    "# --- Save MTrack2 format ---\n",
    "track_output = str(FIGURES_DIR / 'trackresults.txt')\n",
    "write_mtrack2_format(tracks, total_frames, track_output)\n",
    "print(f'\\nTrack data saved to: {track_output}')\n",
    "\n",
    "# --- Track statistics ---\n",
    "lengths = [len(t['positions']) for t in tracks]\n",
    "print(f'\\nTrack statistics:')\n",
    "print(f'  Track length (frames): min={min(lengths)}, max={max(lengths)}, mean={np.mean(lengths):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5 — LOAD & SEGMENT TRACKS\n",
    "# (Functions from Diffusion_Analysis_Corrected.ipynb)\n",
    "# ============================================================================\n",
    "\n",
    "def load_mtrack2_data(file_path):\n",
    "    \"\"\"Load MTrack2 output file and return cleaned data matrix.\"\"\"\n",
    "    my_data = np.genfromtxt(file_path, delimiter='\\t', skip_header=2,\n",
    "                           skip_footer=1, invalid_raise=False)\n",
    "    # Remove Frame column and Flag columns (keep only X, Y pairs)\n",
    "    A = np.zeros(my_data.shape[1] // 3 + 1, dtype=int)\n",
    "    for i in range(my_data.shape[1] // 3 + 1):\n",
    "        A[i] = 3 * i\n",
    "    new_data = np.delete(my_data, A, axis=1)\n",
    "    # Sort data — move NaN values to the end\n",
    "    mask = np.isnan(new_data)\n",
    "    new_mask = np.zeros(mask.shape)\n",
    "    for ind, value in enumerate(mask):\n",
    "        new_mask[ind, :] = ~value * (ind + 1)\n",
    "    new_mask = new_mask.astype(np.int_)\n",
    "    for row_index, row in enumerate(new_mask):\n",
    "        for col_index, item in enumerate(row):\n",
    "            if item == 0:\n",
    "                new_mask[row_index][col_index] = new_mask.shape[0] + 5\n",
    "    for i in range(new_mask.shape[1]):\n",
    "        new_mask[:, i] = np.sort(new_mask[:, i])\n",
    "    data = np.empty((mask.shape[0], mask.shape[1]))\n",
    "    data[:, :] = np.nan\n",
    "    for i in range(new_mask.shape[0]):\n",
    "        for j in range(new_mask.shape[1]):\n",
    "            temp = new_mask[i, j]\n",
    "            if temp < new_mask.shape[0]:\n",
    "                data[i, j] = new_data[temp, j]\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_tracks_at_jumps(data, min_length, max_jump_px):\n",
    "    \"\"\"Extract valid trajectory segments, splitting at large jumps.\"\"\"\n",
    "    segments = []\n",
    "    n_particles = data.shape[1] // 2\n",
    "    segment_id = 0\n",
    "    for i in range(n_particles):\n",
    "        x_col = i * 2\n",
    "        y_col = i * 2 + 1\n",
    "        x_raw = data[:, x_col]\n",
    "        y_raw = data[:, y_col]\n",
    "        valid_mask = ~np.isnan(x_raw) & ~np.isnan(y_raw)\n",
    "        x_clean = x_raw[valid_mask]\n",
    "        y_clean = y_raw[valid_mask]\n",
    "        if len(x_clean) < min_length:\n",
    "            continue\n",
    "        dx = np.diff(x_clean)\n",
    "        dy = np.diff(y_clean)\n",
    "        steps = np.sqrt(dx**2 + dy**2)\n",
    "        bad_jump_indices = np.where(steps > max_jump_px)[0]\n",
    "        if len(bad_jump_indices) == 0:\n",
    "            segment_id += 1\n",
    "            segments.append({'x': x_clean, 'y': y_clean, 'id': segment_id,\n",
    "                             'original_particle': i + 1, 'length': len(x_clean)})\n",
    "        else:\n",
    "            split_indices = bad_jump_indices + 1\n",
    "            x_segments = np.split(x_clean, split_indices)\n",
    "            y_segments = np.split(y_clean, split_indices)\n",
    "            for x_seg, y_seg in zip(x_segments, y_segments):\n",
    "                if len(x_seg) >= min_length:\n",
    "                    segment_id += 1\n",
    "                    segments.append({'x': x_seg, 'y': y_seg, 'id': segment_id,\n",
    "                                     'original_particle': i + 1, 'length': len(x_seg)})\n",
    "    segments.sort(key=lambda s: s['length'], reverse=True)\n",
    "    return segments\n",
    "\n",
    "\n",
    "# ======================== LOAD & SEGMENT ========================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STEP 2: LOAD & SEGMENT TRACKS')\n",
    "print('=' * 70)\n",
    "\n",
    "data = load_mtrack2_data(track_output)\n",
    "print(f'\\nLoaded data: {data.shape[0]} frames, {data.shape[1] // 2} particles')\n",
    "\n",
    "segments = split_tracks_at_jumps(data, MIN_SEGMENT_LENGTH, MAX_JUMP_PX)\n",
    "print(f'Valid segments after splitting: {len(segments)}')\n",
    "\n",
    "if len(segments) == 0:\n",
    "    raise RuntimeError(\"No valid segments! Try lowering MIN_SEGMENT_LENGTH or increasing MAX_JUMP_PX.\")\n",
    "\n",
    "n_to_use = min(NUM_BEST_SEGMENTS, len(segments))\n",
    "selected = segments[:n_to_use]\n",
    "\n",
    "print(f'\\nUsing top {n_to_use} segments (by length):')\n",
    "print(f'  {\"Seg\":<5} {\"Orig Part\":<10} {\"Frames\":<8}')\n",
    "print(f'  {\"-\"*25}')\n",
    "for seg in selected[:10]:\n",
    "    print(f'  {seg[\"id\"]:<5} {seg[\"original_particle\"]:<10} {seg[\"length\"]:<8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6 — PLOT 1: TRAJECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STEP 3: TRAJECTORY PLOT')\n",
    "print('=' * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_to_use))\n",
    "\n",
    "# Left: absolute positions (pixels)\n",
    "ax = axes[0]\n",
    "for seg, c in zip(selected, colors):\n",
    "    ax.plot(seg['x'], seg['y'], '-', linewidth=1, color=c, alpha=0.7)\n",
    "    ax.plot(seg['x'][0], seg['y'][0], 'o', color=c, markersize=4)\n",
    "    ax.plot(seg['x'][-1], seg['y'][-1], 's', color=c, markersize=4)\n",
    "ax.set_xlabel('X (pixels)')\n",
    "ax.set_ylabel('Y (pixels)')\n",
    "ax.set_title(f'Top {n_to_use} Trajectories — Absolute Position')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: displacement from start (microns)\n",
    "ax = axes[1]\n",
    "for seg, c in zip(selected, colors):\n",
    "    x_um = (seg['x'] - seg['x'][0]) * PIXEL_SIZE\n",
    "    y_um = (seg['y'] - seg['y'][0]) * PIXEL_SIZE\n",
    "    ax.plot(x_um, y_um, '-o', markersize=1, linewidth=1, color=c, alpha=0.7)\n",
    "ax.set_xlabel(r'$\\Delta X$ ($\\mu$m)')\n",
    "ax.set_ylabel(r'$\\Delta Y$ ($\\mu$m)')\n",
    "ax.set_title(f'Displacement from Start ({PIXEL_SIZE*1000:.1f} nm/px)')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(str(FIGURES_DIR / 'trajectories.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"trajectories.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7 — PLOT 2: DISPLACEMENT HISTOGRAM + GAUSSIAN FIT\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STEP 4: DISPLACEMENT ANALYSIS')\n",
    "print('=' * 70)\n",
    "\n",
    "# --- Helper functions ---\n",
    "def gaussian(x, amplitude, mean, std_dev):\n",
    "    return amplitude * np.exp(-(x - mean)**2 / (2 * std_dev**2))\n",
    "\n",
    "def compute_chi_squared(observed, expected, errors, n_params):\n",
    "    \"\"\"Compute chi-squared and reduced chi-squared.\"\"\"\n",
    "    valid = errors > 0\n",
    "    if np.sum(valid) <= n_params:\n",
    "        return np.nan, np.nan, 0, np.nan\n",
    "    residuals = (observed[valid] - expected[valid]) / errors[valid]\n",
    "    chi2 = np.sum(residuals**2)\n",
    "    dof = np.sum(valid) - n_params\n",
    "    chi2_red = chi2 / dof if dof > 0 else np.nan\n",
    "    from scipy.stats import chi2 as chi2_dist\n",
    "    pval = 1 - chi2_dist.cdf(chi2, dof) if dof > 0 else np.nan\n",
    "    return chi2, chi2_red, dof, pval\n",
    "\n",
    "# --- Extract displacements ---\n",
    "all_dx = []\n",
    "all_dy = []\n",
    "for seg in selected:\n",
    "    all_dx.extend(np.diff(seg['x']))\n",
    "    all_dy.extend(np.diff(seg['y']))\n",
    "\n",
    "dx_px = np.array(all_dx)\n",
    "dy_px = np.array(all_dy)\n",
    "dx_um = dx_px * PIXEL_SIZE\n",
    "dy_um = dy_px * PIXEL_SIZE\n",
    "n_steps = len(dx_px)\n",
    "\n",
    "print(f'\\nTotal displacement steps: {n_steps}')\n",
    "\n",
    "# --- METHOD 1: Direct Variance ---\n",
    "var_dx = np.var(dx_px)  # px^2\n",
    "var_dy = np.var(dy_px)\n",
    "D_x_direct = var_dx * PIXEL_SIZE**2 / (2 * dt)  # um^2/s\n",
    "D_y_direct = var_dy * PIXEL_SIZE**2 / (2 * dt)\n",
    "D_direct = (D_x_direct + D_y_direct) / 2\n",
    "D_direct_err = abs(D_x_direct - D_y_direct) / 2\n",
    "\n",
    "print(f'\\nMETHOD 1: Direct Variance')\n",
    "print(f'  D_x = {D_x_direct:.4f} um^2/s')\n",
    "print(f'  D_y = {D_y_direct:.4f} um^2/s')\n",
    "print(f'  D_avg = {D_direct:.4f} +/- {D_direct_err:.4f} um^2/s')\n",
    "\n",
    "# --- METHOD 2: Gaussian Fit ---\n",
    "nbins = 20\n",
    "\n",
    "# X direction\n",
    "counts_x, bin_edges_x = np.histogram(dx_um, bins=nbins)\n",
    "bin_centers_x = (bin_edges_x[:-1] + bin_edges_x[1:]) / 2\n",
    "counts_err_x = np.sqrt(np.maximum(counts_x, 1))\n",
    "\n",
    "try:\n",
    "    popt_x, pcov_x = curve_fit(gaussian, bin_centers_x, counts_x,\n",
    "                                p0=[max(counts_x), 0, np.std(dx_um)])\n",
    "    std_x_fit = abs(popt_x[2])\n",
    "    std_x_fit_err = np.sqrt(pcov_x[2, 2]) if pcov_x[2, 2] > 0 else 0\n",
    "    gauss_pred_x = gaussian(bin_centers_x, *popt_x)\n",
    "    chi2_x, chi2_red_x, dof_x, pval_x = compute_chi_squared(\n",
    "        counts_x, gauss_pred_x, counts_err_x, 3)\n",
    "except Exception:\n",
    "    std_x_fit = np.std(dx_um)\n",
    "    std_x_fit_err = 0\n",
    "    popt_x = [max(counts_x), 0, std_x_fit]\n",
    "\n",
    "# Y direction\n",
    "counts_y, bin_edges_y = np.histogram(dy_um, bins=nbins)\n",
    "bin_centers_y = (bin_edges_y[:-1] + bin_edges_y[1:]) / 2\n",
    "counts_err_y = np.sqrt(np.maximum(counts_y, 1))\n",
    "\n",
    "try:\n",
    "    popt_y, pcov_y = curve_fit(gaussian, bin_centers_y, counts_y,\n",
    "                                p0=[max(counts_y), 0, np.std(dy_um)])\n",
    "    std_y_fit = abs(popt_y[2])\n",
    "    std_y_fit_err = np.sqrt(pcov_y[2, 2]) if pcov_y[2, 2] > 0 else 0\n",
    "    gauss_pred_y = gaussian(bin_centers_y, *popt_y)\n",
    "    chi2_y, chi2_red_y, dof_y, pval_y = compute_chi_squared(\n",
    "        counts_y, gauss_pred_y, counts_err_y, 3)\n",
    "except Exception:\n",
    "    std_y_fit = np.std(dy_um)\n",
    "    std_y_fit_err = 0\n",
    "    popt_y = [max(counts_y), 0, std_y_fit]\n",
    "\n",
    "D_x_fit = std_x_fit**2 / (2 * dt)\n",
    "D_y_fit = std_y_fit**2 / (2 * dt)\n",
    "D_fit = (D_x_fit + D_y_fit) / 2\n",
    "D_fit_err = abs(D_x_fit - D_y_fit) / 2\n",
    "\n",
    "print(f'\\nMETHOD 2: Gaussian Fit')\n",
    "print(f'  sigma_x = {std_x_fit:.4f} um, sigma_y = {std_y_fit:.4f} um')\n",
    "print(f'  D_fit = {D_fit:.4f} +/- {D_fit_err:.4f} um^2/s')\n",
    "\n",
    "# --- Plot histogram ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# X histogram\n",
    "ax = axes[0]\n",
    "ax.bar(bin_centers_x, counts_x, width=bin_edges_x[1]-bin_edges_x[0],\n",
    "       alpha=0.6, color='steelblue', label='Data')\n",
    "x_fine = np.linspace(bin_centers_x[0], bin_centers_x[-1], 200)\n",
    "ax.plot(x_fine, gaussian(x_fine, *popt_x), 'r-', linewidth=2,\n",
    "        label=f'Gaussian fit\\n$\\\\sigma$ = {std_x_fit:.4f} $\\\\mu$m')\n",
    "ax.set_xlabel(r'$\\Delta x$ per frame ($\\mu$m)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'X Displacement — D_x = {D_x_fit:.4f} $\\\\mu$m$^2$/s')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Y histogram\n",
    "ax = axes[1]\n",
    "ax.bar(bin_centers_y, counts_y, width=bin_edges_y[1]-bin_edges_y[0],\n",
    "       alpha=0.6, color='darkorange', label='Data')\n",
    "y_fine = np.linspace(bin_centers_y[0], bin_centers_y[-1], 200)\n",
    "ax.plot(y_fine, gaussian(y_fine, *popt_y), 'r-', linewidth=2,\n",
    "        label=f'Gaussian fit\\n$\\\\sigma$ = {std_y_fit:.4f} $\\\\mu$m')\n",
    "ax.set_xlabel(r'$\\Delta y$ per frame ($\\mu$m)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Y Displacement — D_y = {D_y_fit:.4f} $\\\\mu$m$^2$/s')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(str(FIGURES_DIR / 'displacement_histogram.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"displacement_histogram.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8 — PLOT 3: MSD ANALYSIS + ALPHA EXPONENT\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STEP 5: MSD ANALYSIS')\n",
    "print('=' * 70)\n",
    "\n",
    "def linear(t, slope, intercept):\n",
    "    return slope * t + intercept\n",
    "\n",
    "def power_law(t, K, alpha):\n",
    "    return K * t**alpha\n",
    "\n",
    "# --- Compute ensemble MSD ---\n",
    "min_track = min([seg['length'] for seg in selected])\n",
    "max_lag = min(min_track // 2, 30)\n",
    "\n",
    "all_MSDs = []\n",
    "for seg in selected:\n",
    "    x = seg['x'].copy()\n",
    "    y = seg['y'].copy()\n",
    "    MSD_seg = np.zeros(max_lag)\n",
    "    n_frames = len(x)\n",
    "    for lag in range(max_lag):\n",
    "        dx_lag = x[lag+1:] - x[:n_frames-lag-1]\n",
    "        dy_lag = y[lag+1:] - y[:n_frames-lag-1]\n",
    "        r_sq = dx_lag**2 + dy_lag**2\n",
    "        MSD_seg[lag] = np.mean(r_sq) if len(r_sq) > 0 else 0\n",
    "    all_MSDs.append(MSD_seg)\n",
    "\n",
    "all_MSDs = np.array(all_MSDs)\n",
    "MSD_px = np.mean(all_MSDs, axis=0)\n",
    "MSD_err_px = np.std(all_MSDs, axis=0) / np.sqrt(len(selected))\n",
    "\n",
    "MSD_um = MSD_px * PIXEL_SIZE**2\n",
    "MSD_err_um = MSD_err_px * PIXEL_SIZE**2\n",
    "lag_times = np.arange(max_lag) * dt\n",
    "\n",
    "# --- FIT 1: Linear fit for D ---\n",
    "n_fit = max_lag // 4\n",
    "fit_times = lag_times[1:n_fit+1]\n",
    "fit_MSD = MSD_um[1:n_fit+1]\n",
    "fit_err = MSD_err_um[1:n_fit+1]\n",
    "fit_err = np.where(fit_err > 0, fit_err, 1e-10)\n",
    "\n",
    "try:\n",
    "    popt_msd, pcov_msd = curve_fit(linear, fit_times, fit_MSD,\n",
    "                                    sigma=fit_err, absolute_sigma=True, p0=[1, 0])\n",
    "    perr_msd = np.sqrt(np.diag(pcov_msd))\n",
    "    slope = popt_msd[0]\n",
    "    slope_err = perr_msd[0]\n",
    "    intercept = popt_msd[1]\n",
    "    msd_pred = linear(fit_times, *popt_msd)\n",
    "    chi2_msd, chi2_red_msd, dof_msd, pval_msd = compute_chi_squared(\n",
    "        fit_MSD, msd_pred, fit_err, 2)\n",
    "except Exception:\n",
    "    coeffs = np.polyfit(fit_times, fit_MSD, 1)\n",
    "    slope = coeffs[0]\n",
    "    slope_err = 0\n",
    "    intercept = coeffs[1]\n",
    "    msd_pred = linear(fit_times, slope, intercept)\n",
    "    chi2_red_msd = np.nan\n",
    "    pval_msd = np.nan\n",
    "\n",
    "D_msd = slope / 4  # For 2D: MSD = 4*D*t\n",
    "D_msd_err = slope_err / 4\n",
    "\n",
    "print(f'\\nMETHOD 3: MSD Slope')\n",
    "print(f'  Slope = {slope:.6f} +/- {slope_err:.6f} um^2/s')\n",
    "print(f'  D_MSD = {D_msd:.4f} +/- {D_msd_err:.4f} um^2/s')\n",
    "\n",
    "# --- FIT 2: Power-law for alpha ---\n",
    "n_fit_power = max_lag // 2\n",
    "pl_t = lag_times[1:n_fit_power+1]\n",
    "pl_msd = MSD_um[1:n_fit_power+1]\n",
    "pl_err = MSD_err_um[1:n_fit_power+1]\n",
    "pl_err = np.where(pl_err > 0, pl_err, 1e-10)\n",
    "\n",
    "try:\n",
    "    popt_pl, pcov_pl = curve_fit(power_law, pl_t, pl_msd,\n",
    "                                  sigma=pl_err, absolute_sigma=True,\n",
    "                                  p0=[0.001, 1.0], maxfev=5000)\n",
    "    perr_pl = np.sqrt(np.diag(pcov_pl))\n",
    "    K_fit = popt_pl[0]\n",
    "    alpha_fit = popt_pl[1]\n",
    "    K_err = perr_pl[0]\n",
    "    alpha_err = perr_pl[1]\n",
    "except Exception:\n",
    "    log_t = np.log(pl_t)\n",
    "    log_msd = np.log(pl_msd)\n",
    "    coeffs = np.polyfit(log_t, log_msd, 1)\n",
    "    alpha_fit = coeffs[0]\n",
    "    K_fit = np.exp(coeffs[1])\n",
    "    alpha_err = 0\n",
    "    K_err = 0\n",
    "\n",
    "# --- Classify motion ---\n",
    "if alpha_fit > 1.7:\n",
    "    motion_type = 'BALLISTIC / DIRECTED'\n",
    "elif alpha_fit > 1.2:\n",
    "    motion_type = 'SUPERDIFFUSIVE (directed + random)'\n",
    "elif alpha_fit > 0.8:\n",
    "    motion_type = 'DIFFUSIVE (Brownian-like)'\n",
    "else:\n",
    "    motion_type = 'SUBDIFFUSIVE (confined)'\n",
    "\n",
    "print(f'\\nPower-law fit: MSD = K * t^alpha')\n",
    "print(f'  alpha = {alpha_fit:.3f} +/- {alpha_err:.3f}')\n",
    "print(f'  Motion classification: {motion_type}')\n",
    "\n",
    "# --- Plot MSD ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: linear scale\n",
    "ax = axes[0]\n",
    "ax.errorbar(lag_times, MSD_um, yerr=MSD_err_um, fmt='o',\n",
    "            markersize=4, capsize=3, alpha=0.6, color='steelblue', label='MSD data')\n",
    "fit_line_t = np.linspace(0, lag_times[n_fit], 100)\n",
    "ax.plot(fit_line_t, linear(fit_line_t, slope, intercept), 'r-', linewidth=2,\n",
    "        label=f'Linear fit: D = {D_msd:.4f} $\\\\pm$ {D_msd_err:.4f} $\\\\mu$m$^2$/s')\n",
    "pl_line_t = np.linspace(dt, lag_times[n_fit_power], 100)\n",
    "ax.plot(pl_line_t, power_law(pl_line_t, K_fit, alpha_fit), 'g--', linewidth=2,\n",
    "        label=f'Power law: $\\\\alpha$ = {alpha_fit:.2f}')\n",
    "ax.set_xlabel('Lag time $\\\\tau$ (s)', fontsize=12)\n",
    "ax.set_ylabel(r'MSD ($\\mu$m$^2$)', fontsize=12)\n",
    "ax.set_title('MSD — Linear Scale')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: log-log scale\n",
    "ax = axes[1]\n",
    "valid = MSD_um[1:] > 0\n",
    "ax.errorbar(lag_times[1:][valid], MSD_um[1:][valid], yerr=MSD_err_um[1:][valid],\n",
    "            fmt='o', markersize=4, capsize=3, alpha=0.6, color='steelblue', label='MSD data')\n",
    "ref_t = np.logspace(np.log10(dt), np.log10(lag_times[-1]), 50)\n",
    "msd_at_1 = MSD_um[1] if MSD_um[1] > 0 else 1e-6\n",
    "ax.plot(ref_t, msd_at_1 * (ref_t/dt)**1, 'k:', alpha=0.4, linewidth=1.5,\n",
    "        label=r'$\\alpha=1$ (diffusive)')\n",
    "ax.plot(ref_t, msd_at_1 * (ref_t/dt)**2, 'k--', alpha=0.4, linewidth=1.5,\n",
    "        label=r'$\\alpha=2$ (ballistic)')\n",
    "ax.plot(pl_line_t, power_law(pl_line_t, K_fit, alpha_fit), 'g-', linewidth=2,\n",
    "        label=f'Fit: $\\\\alpha$ = {alpha_fit:.2f} $\\\\pm$ {alpha_err:.2f}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Lag time $\\\\tau$ (s)', fontsize=12)\n",
    "ax.set_ylabel(r'MSD ($\\mu$m$^2$)', fontsize=12)\n",
    "ax.set_title('MSD — Log-Log Scale')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(str(FIGURES_DIR / 'msd_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"msd_analysis.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9 — PLOT 4: D COMPARISON (MEASURED vs STOKES-EINSTEIN)\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('STEP 6: STOKES-EINSTEIN COMPARISON')\n",
    "print('=' * 70)\n",
    "\n",
    "# --- Stokes-Einstein theoretical D ---\n",
    "r_m = (BEAD_DIAMETER_UM / 2) * 1e-6  # radius in meters\n",
    "D_theory = k_B * TEMPERATURE / (6 * pi * VISCOSITY * r_m)  # m^2/s\n",
    "D_theory_um = D_theory * 1e12  # um^2/s\n",
    "\n",
    "print(f'\\nStokes-Einstein prediction:')\n",
    "print(f'  D_theory = k_B*T / (6*pi*eta*r)')\n",
    "print(f'  = {k_B:.3e} * {TEMPERATURE:.2f} / (6*pi * {VISCOSITY:.6f} * {r_m:.2e})')\n",
    "print(f'  = {D_theory_um:.4f} um^2/s')\n",
    "\n",
    "print(f'\\n{\"Method\":<20} {\"D (um^2/s)\":<18} {\"Deviation from theory\":<22}')\n",
    "print('-' * 60)\n",
    "for name, D_val, D_err in [('Direct Variance', D_direct, D_direct_err),\n",
    "                             ('Gaussian Fit', D_fit, D_fit_err),\n",
    "                             ('MSD Slope', D_msd, D_msd_err)]:\n",
    "    dev = (D_val - D_theory_um) / D_theory_um * 100\n",
    "    print(f'{name:<20} {D_val:.4f} +/- {D_err:.4f}   {dev:+.1f}%')\n",
    "print(f'{\"Stokes-Einstein\":<20} {D_theory_um:.4f}             (theory)')\n",
    "\n",
    "# --- Bar chart ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "methods = ['Direct\\nVariance', 'Gaussian\\nFit', 'MSD\\nSlope', 'Stokes-\\nEinstein']\n",
    "D_vals = [D_direct, D_fit, D_msd, D_theory_um]\n",
    "D_errs = [D_direct_err, D_fit_err, D_msd_err, 0]\n",
    "bar_colors = ['steelblue', 'darkorange', 'forestgreen', 'crimson']\n",
    "\n",
    "bars = ax.bar(methods, D_vals, yerr=D_errs, capsize=5,\n",
    "              color=bar_colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, D_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(D_vals) * 0.02,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_ylabel(r'Diffusion Coefficient $D$ ($\\mu$m$^2$/s)', fontsize=12)\n",
    "ax.set_title(f'{BEAD_DIAMETER_UM} $\\\\mu$m beads — {GLYCEROL_PERCENT}% glycerol — '\n",
    "             f'{PIXEL_SIZE*1000:.1f} nm/px', fontsize=13)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, max(D_vals) * 1.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(str(FIGURES_DIR / 'D_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"D_comparison.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10 — SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "summary_lines = []\n",
    "summary_lines.append('=' * 70)\n",
    "summary_lines.append('ANALYSIS SUMMARY')\n",
    "summary_lines.append('=' * 70)\n",
    "summary_lines.append(f'')\n",
    "summary_lines.append(f'Video: {avi_path.name}')\n",
    "summary_lines.append(f'Date: {date_folder}')\n",
    "summary_lines.append(f'Bead diameter: {BEAD_DIAMETER_UM} um')\n",
    "summary_lines.append(f'Glycerol: {GLYCEROL_PERCENT}%')\n",
    "summary_lines.append(f'Temperature: {TEMPERATURE_C} C ({TEMPERATURE:.2f} K)')\n",
    "summary_lines.append(f'Viscosity: {VISCOSITY:.6f} Pa.s')\n",
    "summary_lines.append(f'Pixel size: {PIXEL_SIZE} um/px ({PIXEL_SIZE*1000:.1f} nm/px)')\n",
    "summary_lines.append(f'Frame rate: {FRAME_RATE} fps')\n",
    "summary_lines.append(f'')\n",
    "summary_lines.append(f'Tracking:')\n",
    "summary_lines.append(f'  Total frames: {total_frames}')\n",
    "summary_lines.append(f'  Valid tracks: {len(tracks)}')\n",
    "summary_lines.append(f'  Segments used: {n_to_use}')\n",
    "summary_lines.append(f'  Total displacement steps: {n_steps}')\n",
    "summary_lines.append(f'')\n",
    "summary_lines.append(f'Diffusion Coefficients (um^2/s):')\n",
    "summary_lines.append(f'  Method 1 (Direct Variance): {D_direct:.4f} +/- {D_direct_err:.4f}')\n",
    "summary_lines.append(f'  Method 2 (Gaussian Fit):     {D_fit:.4f} +/- {D_fit_err:.4f}')\n",
    "summary_lines.append(f'  Method 3 (MSD Slope):        {D_msd:.4f} +/- {D_msd_err:.4f}')\n",
    "summary_lines.append(f'  Stokes-Einstein (theory):    {D_theory_um:.4f}')\n",
    "summary_lines.append(f'')\n",
    "summary_lines.append(f'Deviations from Stokes-Einstein:')\n",
    "for name, D_val in [('Direct Variance', D_direct),\n",
    "                     ('Gaussian Fit', D_fit),\n",
    "                     ('MSD Slope', D_msd)]:\n",
    "    dev = (D_val - D_theory_um) / D_theory_um * 100\n",
    "    summary_lines.append(f'  {name}: {dev:+.1f}%')\n",
    "summary_lines.append(f'')\n",
    "summary_lines.append(f'MSD Exponent:')\n",
    "summary_lines.append(f'  alpha = {alpha_fit:.3f} +/- {alpha_err:.3f}')\n",
    "summary_lines.append(f'  Classification: {motion_type}')\n",
    "summary_lines.append(f'')\n",
    "summary_lines.append(f'Figures saved to: {FIGURES_DIR}')\n",
    "summary_lines.append('=' * 70)\n",
    "\n",
    "summary_text = '\\n'.join(summary_lines)\n",
    "\n",
    "# Print to notebook\n",
    "print(summary_text)\n",
    "\n",
    "# Save to file\n",
    "with open(str(FIGURES_DIR / 'summary.txt'), 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f'\\nSummary saved to: {FIGURES_DIR / \"summary.txt\"}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}